{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4987f0d4-8686-432b-9089-a9c078548887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the MLP model...\n",
      "Iteration 1, loss = 51.91990347\n",
      "Iteration 2, loss = 6.04483452\n",
      "Iteration 3, loss = 5.06611163\n",
      "Iteration 4, loss = 4.76672442\n",
      "Iteration 5, loss = 4.59725801\n",
      "Iteration 6, loss = 4.48237406\n",
      "Iteration 7, loss = 4.38802219\n",
      "Iteration 8, loss = 4.31103979\n",
      "Iteration 9, loss = 4.23979563\n",
      "Iteration 10, loss = 4.17681570\n",
      "Iteration 11, loss = 4.11262199\n",
      "Iteration 12, loss = 4.05514570\n",
      "Iteration 13, loss = 3.99722483\n",
      "Iteration 14, loss = 3.94065938\n",
      "Iteration 15, loss = 3.88502050\n",
      "Iteration 16, loss = 3.81770666\n",
      "Iteration 17, loss = 3.75793830\n",
      "Iteration 18, loss = 3.69169682\n",
      "Iteration 19, loss = 3.62355850\n",
      "Iteration 20, loss = 3.54939146\n",
      "Iteration 21, loss = 3.47485137\n",
      "Iteration 22, loss = 3.39362811\n",
      "Iteration 23, loss = 3.31632956\n",
      "Iteration 24, loss = 3.23659063\n",
      "Iteration 25, loss = 3.15376209\n",
      "Iteration 26, loss = 3.06306595\n",
      "Iteration 27, loss = 2.97495522\n",
      "Iteration 28, loss = 2.88502845\n",
      "Iteration 29, loss = 2.79939200\n",
      "Iteration 30, loss = 2.70712060\n",
      "Iteration 31, loss = 2.61730662\n",
      "Iteration 32, loss = 2.53331271\n",
      "Iteration 33, loss = 2.45852412\n",
      "Iteration 34, loss = 2.36620858\n",
      "Iteration 35, loss = 2.27767026\n",
      "Iteration 36, loss = 2.18451355\n",
      "Iteration 37, loss = 2.10746674\n",
      "Iteration 38, loss = 2.02469349\n",
      "Iteration 39, loss = 1.94422955\n",
      "Iteration 40, loss = 1.86412994\n",
      "Iteration 41, loss = 1.78093531\n",
      "Iteration 42, loss = 1.70986636\n",
      "Iteration 43, loss = 1.63372349\n",
      "Iteration 44, loss = 1.57312478\n",
      "Iteration 45, loss = 1.48922393\n",
      "Iteration 46, loss = 1.42700479\n",
      "Iteration 47, loss = 1.34549335\n",
      "Iteration 48, loss = 1.30104170\n",
      "Iteration 49, loss = 1.23742935\n",
      "Iteration 50, loss = 1.16659725\n",
      "Iteration 51, loss = 1.09403005\n",
      "Iteration 52, loss = 1.02904254\n",
      "Iteration 53, loss = 0.98257544\n",
      "Iteration 54, loss = 0.91754751\n",
      "Iteration 55, loss = 0.85954346\n",
      "Iteration 56, loss = 0.80772749\n",
      "Iteration 57, loss = 0.76654908\n",
      "Iteration 58, loss = 0.72235503\n",
      "Iteration 59, loss = 0.68036020\n",
      "Iteration 60, loss = 0.64128110\n",
      "Iteration 61, loss = 0.60288977\n",
      "Iteration 62, loss = 0.56127243\n",
      "Iteration 63, loss = 0.51562493\n",
      "Iteration 64, loss = 0.47480654\n",
      "Iteration 65, loss = 0.44355109\n",
      "Iteration 66, loss = 0.40968911\n",
      "Iteration 67, loss = 0.38422610\n",
      "Iteration 68, loss = 0.35401596\n",
      "Iteration 69, loss = 0.32344677\n",
      "Iteration 70, loss = 0.30293919\n",
      "Iteration 71, loss = 0.29279345\n",
      "Iteration 72, loss = 0.62806848\n",
      "Iteration 73, loss = 0.57942405\n",
      "Iteration 74, loss = 0.36830591\n",
      "Iteration 75, loss = 0.26760409\n",
      "Iteration 76, loss = 0.22089835\n",
      "Iteration 77, loss = 0.19347599\n",
      "Iteration 78, loss = 0.17474745\n",
      "Iteration 79, loss = 0.16054504\n",
      "Iteration 80, loss = 0.14989305\n",
      "Iteration 81, loss = 0.14017047\n",
      "Iteration 82, loss = 0.13144545\n",
      "Iteration 83, loss = 0.12388181\n",
      "Iteration 84, loss = 0.11693124\n",
      "Iteration 85, loss = 0.10950500\n",
      "Iteration 86, loss = 0.10499503\n",
      "Iteration 87, loss = 0.09750472\n",
      "Iteration 88, loss = 0.09237399\n",
      "Iteration 89, loss = 0.08795817\n",
      "Iteration 90, loss = 0.08293266\n",
      "Iteration 91, loss = 0.07980589\n",
      "Iteration 92, loss = 0.07552153\n",
      "Iteration 93, loss = 0.07149258\n",
      "Iteration 94, loss = 0.06790255\n",
      "Iteration 95, loss = 0.06396989\n",
      "Iteration 96, loss = 0.06038691\n",
      "Iteration 97, loss = 0.05673914\n",
      "Iteration 98, loss = 0.05431147\n",
      "Iteration 99, loss = 0.05180835\n",
      "Iteration 100, loss = 0.04948085\n",
      "Iteration 101, loss = 0.04738579\n",
      "Iteration 102, loss = 0.04504659\n",
      "Iteration 103, loss = 0.04280093\n",
      "Iteration 104, loss = 0.04107689\n",
      "Iteration 105, loss = 0.04131703\n",
      "Iteration 106, loss = 0.05217729\n",
      "Iteration 107, loss = 0.27862871\n",
      "Iteration 108, loss = 0.93294004\n",
      "Iteration 109, loss = 0.62834707\n",
      "Iteration 110, loss = 0.27532489\n",
      "Iteration 111, loss = 0.14711641\n",
      "Iteration 112, loss = 0.08750538\n",
      "Iteration 113, loss = 0.06262809\n",
      "Iteration 114, loss = 0.05910203\n",
      "Iteration 115, loss = 0.04830420\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training the XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2910: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2981: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Log Loss (ensemble): 4.5969\n",
      "Calculating feature importance from XGBoost...\n",
      "    Feature  Importance\n",
      "0       g-0         0.0\n",
      "588   g-588         0.0\n",
      "577   g-577         0.0\n",
      "578   g-578         0.0\n",
      "579   g-579         0.0\n",
      "580   g-580         0.0\n",
      "581   g-581         0.0\n",
      "582   g-582         0.0\n",
      "583   g-583         0.0\n",
      "584   g-584         0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load data\n",
    "train_features = pd.read_csv(\"train_features.csv\")\n",
    "train_targets = pd.read_csv(\"train_targets_scored.csv\")\n",
    "\n",
    "# Subsample for testing purposes\n",
    "fraction = 0.5\n",
    "train_features = train_features.sample(frac=fraction, random_state=42).reset_index(drop=True)\n",
    "train_targets = train_targets.loc[train_features.index].reset_index(drop=True)\n",
    "\n",
    "# Preprocessing\n",
    "X = train_features.drop(columns=[\"sig_id\"])\n",
    "y = train_targets.drop(columns=[\"sig_id\"])\n",
    "\n",
    "# Encoding of categorical features\n",
    "X = pd.get_dummies(X, columns=[\"cp_type\", \"cp_time\", \"cp_dose\"], drop_first=True)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X[X.columns] = scaler.fit_transform(X[X.columns])\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network Model (MLPClassifier)\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers with 100 and 50 units\n",
    "    activation='relu',  # ReLU activation function\n",
    "    solver='adam',  # Adam optimizer\n",
    "    max_iter=1000,  # Increased number of iterations to allow the model to converge\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train both models\n",
    "print(\"Training the MLP model...\")\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training the XGBoost model...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for both models\n",
    "mlp_pred_prob = mlp_model.predict_proba(X_val)\n",
    "xgb_pred_prob = xgb_model.predict_proba(X_val)\n",
    "\n",
    "# Combine the predictions using averaging (soft voting)\n",
    "# Note: We assume the output probabilities are of shape (n_samples, n_labels)\n",
    "combined_pred_prob = (mlp_pred_prob + xgb_pred_prob) / 2\n",
    "\n",
    "# Calculate log loss for multilabel classification\n",
    "log_loss_score = log_loss(y_val, combined_pred_prob, eps=1e-15)\n",
    "print(f\"Validation Log Loss (ensemble): {log_loss_score:.4f}\")\n",
    "\n",
    "# Feature importance analysis using permutation importance on XGBoost (as it supports feature importance)\n",
    "print(\"Calculating feature importance from XGBoost...\")\n",
    "\n",
    "# Using permutation importance on the XGBoost model\n",
    "result = permutation_importance(xgb_model, X_val, y_val, n_repeats=10, random_state=42)\n",
    "\n",
    "# Sorting and displaying the feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': result.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))  # Displaying top 10 important features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26d44f0-5c96-4fb6-84ce-05c72d528cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the MLP model...\n",
      "Iteration 1, loss = 33.27243452\n",
      "Validation score: 0.418678\n",
      "Iteration 2, loss = 4.58276695\n",
      "Validation score: 0.418678\n",
      "Iteration 3, loss = 4.37525461\n",
      "Validation score: 0.418678\n",
      "Iteration 4, loss = 4.27606034\n",
      "Validation score: 0.418678\n",
      "Iteration 5, loss = 4.19290671\n",
      "Validation score: 0.418678\n",
      "Iteration 6, loss = 4.10397044\n",
      "Validation score: 0.418678\n",
      "Iteration 7, loss = 4.01420491\n",
      "Validation score: 0.418678\n",
      "Iteration 8, loss = 3.92008747\n",
      "Validation score: 0.418678\n",
      "Iteration 9, loss = 3.80943635\n",
      "Validation score: 0.418678\n",
      "Iteration 10, loss = 3.67907091\n",
      "Validation score: 0.418678\n",
      "Iteration 11, loss = 3.51407271\n",
      "Validation score: 0.418678\n",
      "Iteration 12, loss = 3.33405585\n",
      "Validation score: 0.418678\n",
      "Iteration 13, loss = 3.11555705\n",
      "Validation score: 0.417629\n",
      "Iteration 14, loss = 2.88997837\n",
      "Validation score: 0.410283\n",
      "Iteration 15, loss = 2.63678578\n",
      "Validation score: 0.411333\n",
      "Iteration 16, loss = 2.39996863\n",
      "Validation score: 0.412382\n",
      "Iteration 17, loss = 2.15153786\n",
      "Validation score: 0.402938\n",
      "Iteration 18, loss = 1.89876578\n",
      "Validation score: 0.397692\n",
      "Iteration 19, loss = 1.65527247\n",
      "Validation score: 0.395593\n",
      "Iteration 20, loss = 1.44476011\n",
      "Validation score: 0.398741\n",
      "Iteration 21, loss = 1.22878902\n",
      "Validation score: 0.394544\n",
      "Iteration 22, loss = 1.07289007\n",
      "Validation score: 0.381952\n",
      "Validation score did not improve more than tol=0.000100 for 20 consecutive epochs. Stopping.\n",
      "Training the XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2910: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2981: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Log Loss (ensemble): 3.6018\n",
      "Calculating feature importance from XGBoost...\n",
      "    Feature  Importance\n",
      "0       g-0         0.0\n",
      "588   g-588         0.0\n",
      "577   g-577         0.0\n",
      "578   g-578         0.0\n",
      "579   g-579         0.0\n",
      "580   g-580         0.0\n",
      "581   g-581         0.0\n",
      "582   g-582         0.0\n",
      "583   g-583         0.0\n",
      "584   g-584         0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load data\n",
    "train_features = pd.read_csv(\"train_features.csv\")\n",
    "train_targets = pd.read_csv(\"train_targets_scored.csv\")\n",
    "\n",
    "# Subsample for testing purposes\n",
    "fraction = 0.5\n",
    "train_features = train_features.sample(frac=fraction, random_state=42).reset_index(drop=True)\n",
    "train_targets = train_targets.loc[train_features.index].reset_index(drop=True)\n",
    "\n",
    "# Preprocessing\n",
    "X = train_features.drop(columns=[\"sig_id\"])\n",
    "y = train_targets.drop(columns=[\"sig_id\"])\n",
    "\n",
    "# Encoding of categorical features\n",
    "X = pd.get_dummies(X, columns=[\"cp_type\", \"cp_time\", \"cp_dose\"], drop_first=True)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X[X.columns] = scaler.fit_transform(X[X.columns])\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network Model (MLPClassifier)\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),  # Deeper and wider network\n",
    "    activation='relu',  # ReLU activation function\n",
    "    solver='adam',  # Adam optimizer\n",
    "    alpha=0.001,  # L2 regularization\n",
    "    batch_size=64,  # Smaller batch size\n",
    "    learning_rate_init=0.0005,  # Smaller learning rate\n",
    "    max_iter=2000,  # Allow more iterations\n",
    "    early_stopping=True,  # Enable early stopping\n",
    "    n_iter_no_change=20,  # Patience for early stopping\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train both models\n",
    "print(\"Training the MLP model...\")\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training the XGBoost model...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for both models\n",
    "mlp_pred_prob = mlp_model.predict_proba(X_val)\n",
    "xgb_pred_prob = xgb_model.predict_proba(X_val)\n",
    "\n",
    "# Combine the predictions using weighted averaging (soft voting)\n",
    "mlp_weight = 0.6\n",
    "xgb_weight = 0.4\n",
    "combined_pred_prob = mlp_weight * mlp_pred_prob + xgb_weight * xgb_pred_prob\n",
    "\n",
    "# Calculate log loss for multilabel classification\n",
    "log_loss_score = log_loss(y_val, combined_pred_prob, eps=1e-15)\n",
    "print(f\"Validation Log Loss (ensemble): {log_loss_score:.4f}\")\n",
    "\n",
    "# Feature importance analysis using permutation importance on XGBoost (as it supports feature importance)\n",
    "print(\"Calculating feature importance from XGBoost...\")\n",
    "\n",
    "# Using permutation importance on the XGBoost model\n",
    "result = permutation_importance(xgb_model, X_val, y_val, n_repeats=10, random_state=42)\n",
    "\n",
    "# Sorting and displaying the feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': result.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))  # Displaying top 10 important features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bda9e5ee-ec26-4609-8022-1121d1eff8d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples_fit, but n_neighbors = 2, n_samples_fit = 1, n_samples = 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train\u001b[38;5;241m.\u001b[39miloc[:, i])) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     32\u001b[0m     smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, k_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Set k_neighbors to 1\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     X_resampled_label, y_resampled_label \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train_processed, y_train\u001b[38;5;241m.\u001b[39miloc[:, i])  \u001b[38;5;66;03m# Access label correctly using iloc\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Append resampled labels to y_resampled\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     y_resampled\u001b[38;5;241m.\u001b[39mappend(y_resampled_label)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[1;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/over_sampling/_smote/base.py:389\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    386\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[0;32m--> 389\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mkneighbors(X_class, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    390\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[1;32m    391\u001b[0m     X_class, y\u001b[38;5;241m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    392\u001b[0m )\n\u001b[1;32m    393\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/neighbors/_base.py:835\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m         inequality_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors <= n_samples_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 835\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minequality_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_neighbors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, n_samples_fit = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples_fit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# include n_samples for common tests\u001b[39;00m\n\u001b[1;32m    839\u001b[0m     )\n\u001b[1;32m    841\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    842\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples_fit, but n_neighbors = 2, n_samples_fit = 1, n_samples = 1"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing pipeline\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns  # Identify categorical columns\n",
    "numerical_columns = X_train.select_dtypes(exclude=['object']).columns  # Identify numerical columns\n",
    "\n",
    "# Preprocessing steps for numerical and categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_columns),  # Handle missing numerical values\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)  # Handle categorical columns with OneHotEncoder\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to the dataset before SMOTE\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Initialize y_resampled as an empty list\n",
    "y_resampled = []\n",
    "\n",
    "# Resample each label using SMOTE individually with k_neighbors set to 1\n",
    "for i in range(y_train.shape[1]):  # Loop over each label in the multi-label problem\n",
    "    # Check if there is more than one class in the label column\n",
    "    if len(np.unique(y_train.iloc[:, i])) > 1:\n",
    "        smote = SMOTE(random_state=42, k_neighbors=1) \n",
    "        X_resampled_label, y_resampled_label = smote.fit_resample(X_train_processed, y_train.iloc[:, i])  # Access label correctly using iloc\n",
    "        \n",
    "        # Append resampled labels to y_resampled\n",
    "        y_resampled.append(y_resampled_label)\n",
    "    else:\n",
    "        # If there is only one class, append the original label to y_resampled without modification\n",
    "        y_resampled.append(y_train.iloc[:, i].values)\n",
    "\n",
    "# Convert y_resampled to a numpy array after resampling all labels\n",
    "y_resampled = np.array(y_resampled).T  # Transpose so it matches the shape of X_resampled\n",
    "\n",
    "# Now, X_resampled should be used for training the models\n",
    "X_resampled = X_resampled_label  # Assign the X_resampled data to the variable\n",
    "\n",
    "# Define multi-label classifiers\n",
    "mlp_model = MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=100, random_state=42))\n",
    "xgb_model = MultiOutputClassifier(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'))\n",
    "\n",
    "# Train MLP Model\n",
    "print(\"Training MLP model...\")\n",
    "mlp_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', mlp_model)\n",
    "])\n",
    "mlp_pipeline.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Train XGBoost Model\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb_model)\n",
    "])\n",
    "xgb_pipeline.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"Evaluating MLP model...\")\n",
    "mlp_score = mlp_pipeline.score(X_test, y_test)\n",
    "print(f\"MLP Model Accuracy: {mlp_score}\")\n",
    "\n",
    "print(\"Evaluating XGBoost model...\")\n",
    "xgb_score = xgb_pipeline.score(X_test, y_test)\n",
    "print(f\"XGBoost Model Accuracy: {xgb_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e90b2a-c9b5-47d3-8283-63917bcb3f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211df62b-4ad9-441f-b848-d01a7d2517f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
